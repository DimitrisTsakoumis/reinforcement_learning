{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "FErzqoJt6Irx"
      },
      "outputs": [],
      "source": [
        "\n",
        "import math\n",
        "import gym\n",
        "from gym import spaces\n",
        "from gym.utils import seeding\n",
        "import numpy as np\n",
        "\n",
        "class CartPoleSwingUpEnv(gym.Env):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.g = 9.82  # gravity\n",
        "        self.m_c = 0.5  # cart mass\n",
        "        self.m_p = 0.5  # pendulum mass\n",
        "        self.total_m = (self.m_p + self.m_c)\n",
        "        self.l = 0.6 # pole's length\n",
        "        self.m_p_l = (self.m_p*self.l)\n",
        "        self.force_mag = 10.0\n",
        "        self.dt = 0.01  # seconds between state updates\n",
        "        self.b = 0.1  # friction coefficient\n",
        "\n",
        "        self.t = 0 # timestep\n",
        "        self.t_limit = 500\n",
        "\n",
        "        # Angle at which to fail the episode\n",
        "        self.theta_threshold_radians = 12 * 2 * math.pi / 360\n",
        "        self.x_threshold = 2.4\n",
        "\n",
        "        high = np.array([\n",
        "            np.finfo(np.float32).max,\n",
        "            np.finfo(np.float32).max,\n",
        "            np.finfo(np.float32).max,\n",
        "            np.finfo(np.float32).max,\n",
        "            np.finfo(np.float32).max])\n",
        "\n",
        "        self.action_space = spaces.Discrete(2)\n",
        "        self.observation_space = spaces.Box(-high, high)\n",
        "\n",
        "        self._seed()\n",
        "        self.viewer = None\n",
        "        self.state = None\n",
        "\n",
        "    def _seed(self, seed=None):\n",
        "        self.np_random, seed = seeding.np_random(seed)\n",
        "        return [seed]\n",
        "\n",
        "    def step(self, action):\n",
        "        # Valid action\n",
        "        # action = np.clip(action, -1.0, 1.0)[0]\n",
        "        # action *= self.force_mag\n",
        "        if action == 0:\n",
        "            action = -1\n",
        "        action *= (self.force_mag)\n",
        "\n",
        "        state = self.state\n",
        "        x, x_dot, theta, theta_dot = state\n",
        "\n",
        "        s = math.sin(theta)\n",
        "        c = math.cos(theta)\n",
        "\n",
        "        xdot_update = (-2*self.m_p_l*(theta_dot**2)*s + 3*self.m_p*self.g*s*c + 4*action - 4*self.b*x_dot)/(4*self.total_m - 3*self.m_p*c**2)\n",
        "        thetadot_update = (-3*self.m_p_l*(theta_dot**2)*s*c + 6*self.total_m*self.g*s + 6*(action - self.b*x_dot)*c)/(4*self.l*self.total_m - 3*self.m_p_l*c**2)\n",
        "        x = x + x_dot*self.dt\n",
        "        theta = theta + theta_dot*self.dt\n",
        "        x_dot = x_dot + xdot_update*self.dt\n",
        "        theta_dot = theta_dot + thetadot_update*self.dt\n",
        "\n",
        "        self.state = (x,x_dot,theta,theta_dot)\n",
        "\n",
        "        done = False\n",
        "        dead = False\n",
        "        if  x < -self.x_threshold or x > self.x_threshold:\n",
        "            dead = True\n",
        "            done = True\n",
        "\n",
        "        self.t += 1\n",
        "\n",
        "        if self.t >= self.t_limit:\n",
        "            done = True\n",
        "\n",
        "        two_pi = 2 * np.pi\n",
        "        reward_theta = (np.e**(np.cos(theta)+1.0)-1.0)\n",
        "        reward_x = np.cos((x/self.x_threshold)*(np.pi/2.0))\n",
        "        reward_theta_dot = (np.cos(theta) * (np.e**(np.cos(theta_dot)+1.0)-1.0) /  two_pi) + 1.0\n",
        "        reward_x_dot =  ((np.cos(theta) * (np.e**(np.cos(x_dot)+1.0) - 1) / two_pi) + 1.0)\n",
        "        reward = (reward_theta + reward_x + reward_theta_dot + reward_x_dot) / 4.0\n",
        "\n",
        "        if dead:\n",
        "            reward = reward * (1.0 - ((self.t_limit - self.t) / self.t_limit))\n",
        "\n",
        "\n",
        "        obs = np.array([x,x_dot,np.cos(theta),np.sin(theta),theta_dot])\n",
        "\n",
        "        return obs, reward, done, {}\n",
        "\n",
        "    def reset(self):\n",
        "        #self.state = self.np_random.normal(loc=np.array([0.0, 0.0, 30*(2*np.pi)/360, 0.0]), scale=np.array([0.0, 0.0, 0.0, 0.0]))\n",
        "        self.state = np.random.normal(loc=np.array([0.0, 0.0, np.pi, 0.0]), scale=np.array([0.2, 0.2, 0.2, 0.2]))\n",
        "        self.steps_beyond_done = None\n",
        "        self.t = 0 # timestep\n",
        "        x, x_dot, theta, theta_dot = self.state\n",
        "        obs = np.array([x,x_dot,np.cos(theta),np.sin(theta),theta_dot])\n",
        "        return obs\n",
        "\n",
        "    def render(self, mode='human', close=False):\n",
        "        if close:\n",
        "            if self.viewer is not None:\n",
        "                self.viewer.close()\n",
        "                self.viewer = None\n",
        "            return"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "\n",
        "class A2C(nn.Module):\n",
        "\n",
        "    def __init__(self, env, hidden_size=128, gamma=.99, random_seed=None):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        if random_seed:\n",
        "            env.seed(random_seed)\n",
        "            torch.manual_seed(random_seed)\n",
        "\n",
        "        self.env = env\n",
        "        self.gamma = gamma\n",
        "        self.hidden_size = hidden_size\n",
        "        self.in_size = len(env.observation_space.sample().flatten())\n",
        "        self.out_size = self.env.action_space.n\n",
        "\n",
        "        self.actor = nn.Sequential(\n",
        "            nn.Linear(self.in_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, self.out_size)\n",
        "        ).double()\n",
        "\n",
        "        self.critic = nn.Sequential(\n",
        "            nn.Linear(self.in_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, 1)\n",
        "        ).double()\n",
        "\n",
        "    def train_env_episode(self, render=False):\n",
        "\n",
        "        rewards = []\n",
        "        critic_vals = []\n",
        "        action_lp_vals = []\n",
        "\n",
        "        # Run episode and save information\n",
        "\n",
        "        observation = self.env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            if render:\n",
        "                self.env.render()\n",
        "\n",
        "            observation = torch.from_numpy(observation).double()\n",
        "\n",
        "            # Get action from actor\n",
        "            action_logits = self.actor(observation)\n",
        "\n",
        "            action = Categorical(logits=action_logits).sample()\n",
        "\n",
        "            # Get action probability\n",
        "            action_log_prob = action_logits[action]\n",
        "\n",
        "            # Get value from critic\n",
        "            pred = torch.squeeze(self.critic(observation).view(-1))\n",
        "\n",
        "            # Write prediction and action/probabilities to arrays\n",
        "            action_lp_vals.append(action_log_prob)\n",
        "            critic_vals.append(pred)\n",
        "\n",
        "            # Send action to environment and get rewards, next state\n",
        "\n",
        "            observation, reward, done, info = self.env.step(action.item())\n",
        "            rewards.append(torch.tensor(reward).double())\n",
        "\n",
        "        total_reward = sum(rewards)\n",
        "\n",
        "        # Convert reward array to expected return and standardize\n",
        "        for t_i in range(len(rewards)):\n",
        "\n",
        "            for t in range(t_i + 1, len(rewards)):\n",
        "                rewards[t_i] += rewards[t] * (self.gamma ** (t_i - t))\n",
        "\n",
        "        # Convert output arrays to tensors using torch.stack\n",
        "        def f(inp):\n",
        "            return torch.stack(tuple(inp), 0)\n",
        "\n",
        "        # Standardize rewards\n",
        "        rewards = f(rewards)\n",
        "        rewards = (rewards - torch.mean(rewards)) / (torch.std(rewards) + .000000000001)\n",
        "\n",
        "        return rewards, f(critic_vals), f(action_lp_vals), total_reward\n",
        "\n",
        "    def test_env_episode(self, render=True):\n",
        "        observation = self.env.reset()\n",
        "        rewards = []\n",
        "        done = False\n",
        "        while not done:\n",
        "\n",
        "            if render:\n",
        "                self.env.render()\n",
        "\n",
        "            observation = torch.from_numpy(observation).double()\n",
        "\n",
        "            # Get action from actor\n",
        "            action_logits = self.actor(observation)\n",
        "            action = Categorical(logits=action_logits).sample()\n",
        "\n",
        "            observation, reward, done, info = self.env.step(action.item())\n",
        "            rewards.append(reward)\n",
        "\n",
        "        return sum(rewards)\n",
        "\n",
        "    @staticmethod\n",
        "    def compute_loss(action_p_vals, G, V, critic_loss=nn.SmoothL1Loss()):\n",
        "        \"\"\"\n",
        "        Actor Advantage Loss, where advantage = G - V\n",
        "        Critic Loss, using mean squared error\n",
        "        :param critic_loss: loss function for critic   :Pytorch loss module\n",
        "        :param action_p_vals: Action Log Probabilities  :Tensor\n",
        "        :param G: Actual Expected Returns   :Tensor\n",
        "        :param V: Predicted Expected Returns    :Tensor\n",
        "        :return: Actor loss tensor, Critic loss tensor  :Tensor\n",
        "        \"\"\"\n",
        "        assert len(action_p_vals) == len(G) == len(V)\n",
        "        advantage = G - V.detach()\n",
        "        return -(torch.sum(action_p_vals * advantage)), critic_loss(G, V)"
      ],
      "metadata": {
        "id": "qo89iK166NPh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Cart-pole reinforcement learning environment:\n",
        "Agent learns to balance a pole on a cart\n",
        "\n",
        "a2c: Agent uses Advantage Actor Critic algorithm\n",
        "\n",
        "\"\"\"\n",
        "import gym\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "LR = .01  # Learning rate\n",
        "SEED = None  # Random seed for reproducibility\n",
        "MAX_EPISODES = 10000  # Max number of episodes\n",
        "\n",
        "# Init actor-critic agent\n",
        "agent = A2C(CartPoleSwingUpEnv(), random_seed=SEED)\n",
        "\n",
        "# Init optimizers\n",
        "actor_optim = optim.Adam(agent.actor.parameters(), lr=LR)\n",
        "critic_optim = optim.Adam(agent.critic.parameters(), lr=LR)\n",
        "\n",
        "\n",
        "# Training\n",
        "\n",
        "r = []  # Array containing total rewards\n",
        "avg_r = 0  # Value storing average reward over last 100 episodes\n",
        "\n",
        "for i in range(MAX_EPISODES):\n",
        "    critic_optim.zero_grad()\n",
        "    actor_optim.zero_grad()\n",
        "\n",
        "    rewards, critic_vals, action_lp_vals, total_reward = agent.train_env_episode(render=False)\n",
        "    r.append(total_reward)\n",
        "\n",
        "    l_actor, l_critic = agent.compute_loss(action_p_vals=action_lp_vals, G=rewards, V=critic_vals)\n",
        "\n",
        "    l_actor.backward()\n",
        "    l_critic.backward()\n",
        "\n",
        "    actor_optim.step()\n",
        "    critic_optim.step()\n",
        "\n",
        "    # Check average reward every 100 episodes, print, and end script if solved\n",
        "    if len(r) >= 100:  # check average every 100 episodes\n",
        "\n",
        "        episode_count = i - (i % 100)\n",
        "        prev_episodes = r[len(r) - 100:]\n",
        "        avg_r = sum(prev_episodes) / len(prev_episodes)\n",
        "        if len(r) % 100 == 0:\n",
        "            print(f'Average reward during episodes {episode_count}-{episode_count+100} is {avg_r.item()}')\n",
        "        if avg_r > 195:\n",
        "            print(f\"Solved CartPole-v0 with average reward {avg_r.item()}\")\n",
        "            break\n",
        "\n",
        "# Test\n",
        "\n",
        "for _ in range(50):\n",
        "    agent.test_env_episode(render=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9weJ4ZqR6NRb",
        "outputId": "38e44141-b208-4bc5-e385-dff0913834a9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average reward during episodes 0-100 is 135.933780412094\n",
            "Average reward during episodes 100-200 is 92.61908109548132\n",
            "Average reward during episodes 200-300 is 196.56599982665898\n",
            "Solved CartPole-v0 with average reward 196.56599982665898\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n7vXWfDQ6NTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9Y2gqaxF6NWs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}